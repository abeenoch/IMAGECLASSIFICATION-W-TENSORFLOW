{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading CIFAR-10 dataset\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import unittest as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "#removed the test.tes bla bla bla,remember to commit that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_names():\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape and transpose\n",
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        # note the encoding type is 'latin1'\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring the data\n",
    "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
    "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
    "    \n",
    "    if not (0 <= sample_id < len(features)):\n",
    "        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))\n",
    "        return None\n",
    "\n",
    "    print('\\nStats of batch #{}:'.format(batch_id))\n",
    "    print('# of Samples: {}\\n'.format(len(features)))\n",
    "    \n",
    "    label_names = load_label_names()\n",
    "    label_counts = dict(zip(*np.unique(labels, return_counts=True)))\n",
    "    for key, value in label_counts.items():\n",
    "        print('Label Counts of [{}]({}) : {}'.format(key, label_names[key].upper(), value))\n",
    "    \n",
    "    sample_image = features[sample_id]\n",
    "    sample_label = labels[sample_id]\n",
    "    \n",
    "    print('\\nExample of Image {}:'.format(sample_id))\n",
    "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
    "    print('Image - Shape: {}'.format(sample_image.shape))\n",
    "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
    "    \n",
    "    plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch #3:\n",
      "# of Samples: 10000\n",
      "\n",
      "Label Counts of [0](AIRPLANE) : 994\n",
      "Label Counts of [1](AUTOMOBILE) : 1042\n",
      "Label Counts of [2](BIRD) : 965\n",
      "Label Counts of [3](CAT) : 997\n",
      "Label Counts of [4](DEER) : 990\n",
      "Label Counts of [5](DOG) : 1029\n",
      "Label Counts of [6](FROG) : 978\n",
      "Label Counts of [7](HORSE) : 1015\n",
      "Label Counts of [8](SHIP) : 961\n",
      "Label Counts of [9](TRUCK) : 1029\n",
      "\n",
      "Example of Image 7000:\n",
      "Image - Min Value: 24 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 0 Name: airplane\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 7000\n",
    "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing-normalize\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: input image data in numpy array [32, 32, 3]\n",
    "        return\n",
    "            - normalized x \n",
    "    \"\"\"\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    x = (x-min_val) / (max_val-min_val)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding \n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: a list of labels\n",
    "        return\n",
    "            - one hot encoding matrix (number of labels, number of class)\n",
    "    \"\"\"\n",
    "    encoded = np.zeros((len(x), 10))\n",
    "    \n",
    "    for idx, val in enumerate(x):\n",
    "        encoded[idx][val] = 1\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
    "    features = normalize(features)\n",
    "    labels = one_hot_encode(labels)\n",
    "\n",
    "    pickle.dump((features, labels), open(filename, 'wb'))\n",
    "\n",
    "\n",
    "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
    "    n_batches = 5\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for batch_i in range(1, n_batches + 1):\n",
    "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
    "        \n",
    "        # find index to be the point as validation data in the whole dataset of the batch (10%)\n",
    "        index_of_validation = int(len(features) * 0.1)\n",
    "\n",
    "        # preprocess the 90% of the whole dataset of the batch\n",
    "        # - normalize the features\n",
    "        # - one_hot_encode the lables\n",
    "        # - save in a new file named, \"preprocess_batch_\" + batch_number\n",
    "        # - each file for each batch\n",
    "        _preprocess_and_save(normalize, one_hot_encode,\n",
    "                             features[:-index_of_validation], labels[:-index_of_validation], \n",
    "                             'preprocess_batch_' + str(batch_i) + '.p')\n",
    "          # unlike the training dataset, validation dataset will be added through all batch dataset\n",
    "        # - take 10% of the whold dataset of the batch\n",
    "        # - add them into a list of\n",
    "        #   - valid_features\n",
    "        #   - valid_labels\n",
    "        valid_features.extend(features[-index_of_validation:])\n",
    "        valid_labels.extend(labels[-index_of_validation:])\n",
    "\n",
    "    # preprocess the all stacked validation dataset\n",
    "    _preprocess_and_save(normalize, one_hot_encode,\n",
    "                         np.array(valid_features), np.array(valid_labels),\n",
    "                         'preprocess_validation.p')\n",
    "\n",
    "    # load the test dataset\n",
    "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    # preprocess the testing data\n",
    "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    test_labels = batch['labels']\n",
    "\n",
    "    # Preprocess and Save all testing data\n",
    "    _preprocess_and_save(normalize, one_hot_encode,\n",
    "                         np.array(test_features), np.array(test_labels),\n",
    "                         'preprocess_training.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint\n",
    "import pickle\n",
    "\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing input for tensorflow model\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "import tensorflow as tf \n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "# Inputs\n",
    "x = tf.compat.v1.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.compat.v1.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "keep_prob = tf.compat.v1.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Acer\\miniconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.executing_eagerly()\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# -initializer = tf.contrib.layers.xavier_initializer(seed=1)\n",
    "initializer = tf.compat.v1.truncated_normal_initializer(stddev=0.1)\n",
    "\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    conv1_filter = tf.Variable(tf.compat.v1.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    conv2_filter = tf.Variable(tf.compat.v1.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "    conv3_filter = tf.Variable(tf.compat.v1.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.08))\n",
    "    conv4_filter = tf.Variable(tf.compat.v1.truncated_normal(shape=[5, 5, 256, 512], mean=0, stddev=0.08))\n",
    "\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1_bn = tf.compat.v1.layers.batch_normalization(conv1_pool)\n",
    "\n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
    "    conv2_bn = tf.compat.v1.layers.batch_normalization(conv2_pool)\n",
    "  \n",
    "    # 5, 6\n",
    "    conv3 = tf.nn.conv2d(conv2_bn, conv3_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  \n",
    "    conv3_bn = tf.compat.v1.layers.batch_normalization(conv3_pool)\n",
    "    \n",
    "    # 7, 8\n",
    "    conv4 = tf.nn.conv2d(conv3_bn, conv4_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "    conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv4_bn = tf.compat.v1.layers.batch_normalization(conv4_pool)\n",
    "    \n",
    "      # 9\n",
    "      \n",
    "    \n",
    "    flat = tf.contrib.layers.flatten(conv4_bn)  \n",
    "\n",
    "    # 10\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "    full1 = tf.nn.dropout(full1, keep_prob)\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "    # 11\n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "    full2 = tf.nn.dropout(full2, keep_prob)\n",
    "    full2 = tf.v1.layers.batch_normalization(full2)\n",
    "    \n",
    "    # 12\n",
    "    full3 = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=tf.nn.relu)\n",
    "    full3 = tf.nn.dropout(full3, keep_prob)\n",
    "    full3 = tf.layers.batch_normalization(full3)    \n",
    "    \n",
    "    # 13\n",
    "    full4 = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=1024, activation_fn=tf.nn.relu)\n",
    "    full4 = tf.nn.dropout(full4, keep_prob)\n",
    "    full4 = tf.layers.batch_normalization(full4)        \n",
    "    \n",
    "    # 14\n",
    "    out = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=10, activation_fn=None)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "keep_probability = 0.7\n",
    "import tensorflow as tf\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_57316\\1661218341.py:20: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  conv1_bn = tf.compat.v1.layers.batch_normalization(conv1_pool)\n",
      "c:\\Users\\Acer\\miniconda3\\lib\\site-packages\\keras\\legacy_tf_layers\\normalization.py:463: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  return layer.apply(inputs, training=training)\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_57316\\1661218341.py:26: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  conv2_bn = tf.compat.v1.layers.batch_normalization(conv2_pool)\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_57316\\1661218341.py:32: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  conv3_bn = tf.compat.v1.layers.batch_normalization(conv3_pool)\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_57316\\1661218341.py:38: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  conv4_bn = tf.compat.v1.layers.batch_normalization(conv4_pool)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Acer\\imageclassification.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000015?line=0'>1</a>\u001b[0m logits \u001b[39m=\u001b[39m conv_net(x, keep_prob)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000015?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39midentity(logits, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# Name logits Tensor, so that can be loaded from disk after training\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000015?line=3'>4</a>\u001b[0m \u001b[39m# Loss and Optimizer\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Acer\\imageclassification.ipynb Cell 14'\u001b[0m in \u001b[0;36mconv_net\u001b[1;34m(x, keep_prob)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000013?line=37'>38</a>\u001b[0m conv4_bn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mbatch_normalization(conv4_pool)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000013?line=39'>40</a>\u001b[0m   \u001b[39m# 9\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000013?line=42'>43</a>\u001b[0m flat \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcontrib\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mflatten(conv4_bn)  \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000013?line=44'>45</a>\u001b[0m \u001b[39m# 10\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000013?line=45'>46</a>\u001b[0m full1 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcontrib\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mfully_connected(inputs\u001b[39m=\u001b[39mflat, num_outputs\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, activation_fn\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mrelu)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "logits = conv_net(x, keep_prob)\n",
    "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer, \n",
    "                feed_dict={\n",
    "                    x: feature_batch,\n",
    "                    y: label_batch,\n",
    "                    keep_prob: keep_probability\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    loss_val, acc = sess.run([loss,accuracy],\n",
    "                    feed_dict={\n",
    "                        x: feature_batch,\n",
    "                        y: label_batch,\n",
    "                        keep_prob: 1.\n",
    "                    })\n",
    "    valid_acc = sess.run(accuracy, \n",
    "                         feed_dict={\n",
    "                             x: valid_features,\n",
    "                             y: valid_labels,\n",
    "                             keep_prob: 1.\n",
    "                         })\n",
    "    \n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]\n",
    "\n",
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, mode='rb'))\n",
    "\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return batch_features_labels(features, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument `fetch` = 0.001 has invalid type \"float\" must be a string or Tensor. (Can not convert a float into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Acer\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:304\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=302'>303</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=303'>304</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches\u001b[39m.\u001b[39mappend(ops\u001b[39m.\u001b[39;49mget_default_graph()\u001b[39m.\u001b[39;49mas_graph_element(\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=304'>305</a>\u001b[0m       fetch, allow_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, allow_operation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=305'>306</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Acer\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3974\u001b[0m, in \u001b[0;36mGraph.as_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/framework/ops.py?line=3972'>3973</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/framework/ops.py?line=3973'>3974</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_as_graph_element_locked(obj, allow_tensor, allow_operation)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:4062\u001b[0m, in \u001b[0;36mGraph._as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/framework/ops.py?line=4059'>4060</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/framework/ops.py?line=4060'>4061</a>\u001b[0m   \u001b[39m# We give up!\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/framework/ops.py?line=4061'>4062</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan not convert a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m into a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/framework/ops.py?line=4062'>4063</a>\u001b[0m                   (\u001b[39mtype\u001b[39m(obj)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, types_str))\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a float into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Acer\\imageclassification.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000019?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_batches \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000019?line=16'>17</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_features, batch_labels \u001b[39min\u001b[39;00m load_preprocess_training_batch(batch_i, batch_size):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000019?line=17'>18</a>\u001b[0m         train_neural_network(sess, learning_rate, keep_probability, batch_features, batch_labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000019?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{:>2}\u001b[39;00m\u001b[39m, CIFAR-10 Batch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:  \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, batch_i), end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000019?line=20'>21</a>\u001b[0m     print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
      "\u001b[1;32mc:\\Users\\Acer\\imageclassification.ipynb Cell 17'\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[1;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000016?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_neural_network\u001b[39m(session, optimizer, keep_probability, feature_batch, label_batch):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000016?line=1'>2</a>\u001b[0m     session\u001b[39m.\u001b[39;49mrun(optimizer, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000016?line=2'>3</a>\u001b[0m                 feed_dict\u001b[39m=\u001b[39;49m{\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000016?line=3'>4</a>\u001b[0m                     x: feature_batch,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000016?line=4'>5</a>\u001b[0m                     y: label_batch,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000016?line=5'>6</a>\u001b[0m                     keep_prob: keep_probability\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Acer/imageclassification.ipynb#ch0000016?line=6'>7</a>\u001b[0m                 })\n",
      "File \u001b[1;32mc:\\Users\\Acer\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:967\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=963'>964</a>\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=965'>966</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=966'>967</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=967'>968</a>\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=968'>969</a>\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=969'>970</a>\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1175\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=1171'>1172</a>\u001b[0m       feed_map[compat\u001b[39m.\u001b[39mas_bytes(subfeed_t\u001b[39m.\u001b[39mname)] \u001b[39m=\u001b[39m (subfeed_t, subfeed_val)\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=1173'>1174</a>\u001b[0m \u001b[39m# Create a fetch handler to take care of the structure of fetches.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=1174'>1175</a>\u001b[0m fetch_handler \u001b[39m=\u001b[39m _FetchHandler(\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=1175'>1176</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph, fetches, feed_dict_tensor, feed_handles\u001b[39m=\u001b[39;49mfeed_handles)\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=1177'>1178</a>\u001b[0m \u001b[39m# Run request and get response.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=1178'>1179</a>\u001b[0m \u001b[39m# We need to keep the returned movers alive for the following _do_run().\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=1179'>1180</a>\u001b[0m \u001b[39m# These movers are no longer needed when _do_run() completes, and\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=1180'>1181</a>\u001b[0m \u001b[39m# are deleted when `movers` goes out of scope when this _run() ends.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=1181'>1182</a>\u001b[0m \u001b[39m# TODO(yuanbyu, keveman): Revisit whether we should just treat feeding\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=1182'>1183</a>\u001b[0m \u001b[39m# of a handle from a different device as an error.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=1183'>1184</a>\u001b[0m _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_with_movers(feed_dict_tensor, feed_map)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:485\u001b[0m, in \u001b[0;36m_FetchHandler.__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=472'>473</a>\u001b[0m \u001b[39m\"\"\"Creates a fetch handler.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=473'>474</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=474'>475</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=481'>482</a>\u001b[0m \u001b[39m    direct feeds.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=482'>483</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=483'>484</a>\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m--> <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=484'>485</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetch_mapper \u001b[39m=\u001b[39m _FetchMapper\u001b[39m.\u001b[39;49mfor_fetch(fetches)\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=485'>486</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetches \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=486'>487</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_targets \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Acer\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:276\u001b[0m, in \u001b[0;36m_FetchMapper.for_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=273'>274</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fetch, tensor_type):\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=274'>275</a>\u001b[0m       fetches, contraction_fn \u001b[39m=\u001b[39m fetch_fn(fetch)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=275'>276</a>\u001b[0m       \u001b[39mreturn\u001b[39;00m _ElementFetchMapper(fetches, contraction_fn)\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=276'>277</a>\u001b[0m \u001b[39m# Did not find anything.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=277'>278</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m has invalid type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=278'>279</a>\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(fetch)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:307\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=303'>304</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches\u001b[39m.\u001b[39mappend(ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39mas_graph_element(\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=304'>305</a>\u001b[0m       fetch, allow_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_operation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=305'>306</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=306'>307</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m has invalid type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=307'>308</a>\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(fetch)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m must be a string or Tensor. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=308'>309</a>\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=309'>310</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=310'>311</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m cannot be interpreted as \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/client/session.py?line=311'>312</a>\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma Tensor. (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument `fetch` = 0.001 has invalid type \"float\" must be a string or Tensor. (Can not convert a float into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.compat.v1.Session()\n",
    "tf.compat.v1.global_variables_initializer()\n",
    "from tensorflow.python.keras import optimizers as opt\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, learning_rate, keep_probability, batch_features, batch_labels)\n",
    "                \n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]\n",
    "\n",
    "def display_image_predictions(features, labels, predictions, top_n_predictions):\n",
    "    n_classes = 10\n",
    "    label_names = load_label_names()\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(range(n_classes))\n",
    "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
    "\n",
    "    fig, axies = plt.subplots(nrows=top_n_predictions, ncols=2, figsize=(20, 10))\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
    "\n",
    "\n",
    "    n_predictions = 3\n",
    "    margin = 0.05\n",
    "    ind = np.arange(n_predictions)\n",
    "    width = (1. - 2. * margin) / n_predictions\n",
    "   \n",
    "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n",
    "        if (image_i < top_n_predictions):\n",
    "            pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
    "            correct_name = label_names[label_id]\n",
    "            \n",
    "            axies[image_i][0].imshow((feature*255).astype(np.int32, copy=False))\n",
    "            axies[image_i][0].set_title(correct_name)\n",
    "            axies[image_i][0].set_axis_off()\n",
    "\n",
    "            axies[image_i][1].barh(ind + margin, pred_values[:3], width)\n",
    "            axies[image_i][1].set_yticks(ind + margin)\n",
    "            axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
    "            axies[image_i][1].set_xticks([0, 0.5, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File does not exist. Received: {filename}.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 48>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m         random_test_predictions \u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39mrun(\n\u001b[0;32m     43\u001b[0m             tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mtop_k(tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msoftmax(loaded_logits), top_n_predictions),\n\u001b[0;32m     44\u001b[0m             feed_dict\u001b[39m=\u001b[39m{loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: \u001b[39m1.0\u001b[39m})\n\u001b[0;32m     45\u001b[0m         display_image_predictions(random_test_features, random_test_labels, random_test_predictions, top_n_predictions)\n\u001b[1;32m---> 48\u001b[0m test_model()\n",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m loaded_graph \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mGraph()\n\u001b[0;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mSession(graph\u001b[39m=\u001b[39mloaded_graph) \u001b[39mas\u001b[39;00m sess:\n\u001b[0;32m     18\u001b[0m     \u001b[39m# Load model\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     loader \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mtrain\u001b[39m.\u001b[39;49mimport_meta_graph(save_model_path \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.meta\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     20\u001b[0m     loader\u001b[39m.\u001b[39mrestore(sess, save_model_path)\n\u001b[0;32m     22\u001b[0m     \u001b[39m# Get Tensors from loaded model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1565\u001b[0m, in \u001b[0;36mimport_meta_graph\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1454'>1455</a>\u001b[0m \u001b[39m@tf_export\u001b[39m(v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtrain.import_meta_graph\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1455'>1456</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimport_meta_graph\u001b[39m(meta_graph_or_file,\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1456'>1457</a>\u001b[0m                       clear_devices\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1457'>1458</a>\u001b[0m                       import_scope\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1458'>1459</a>\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1459'>1460</a>\u001b[0m   \u001b[39m\"\"\"Recreates a Graph saved in a `MetaGraphDef` proto.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1460'>1461</a>\u001b[0m \n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1461'>1462</a>\u001b[0m \u001b[39m  This function takes a `MetaGraphDef` protocol buffer as input. If\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1562'>1563</a>\u001b[0m \u001b[39m  @end_compatibility\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1563'>1564</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m  \u001b[39m# pylint: disable=g-doc-exception\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1564'>1565</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _import_meta_graph_with_return_elements(meta_graph_or_file,\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1565'>1566</a>\u001b[0m                                                  clear_devices, import_scope,\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1566'>1567</a>\u001b[0m                                                  \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Acer\\miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1581\u001b[0m, in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1576'>1577</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExporting/importing meta graphs is not supported when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1577'>1578</a>\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39meager execution is enabled. No graph exists when eager \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1578'>1579</a>\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mexecution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1579'>1580</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(meta_graph_or_file, meta_graph_pb2\u001b[39m.\u001b[39mMetaGraphDef):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1580'>1581</a>\u001b[0m   meta_graph_def \u001b[39m=\u001b[39m meta_graph\u001b[39m.\u001b[39;49mread_meta_graph_file(meta_graph_or_file)\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1581'>1582</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/training/saver.py?line=1582'>1583</a>\u001b[0m   meta_graph_def \u001b[39m=\u001b[39m meta_graph_or_file\n",
      "File \u001b[1;32mc:\\Users\\Acer\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py:634\u001b[0m, in \u001b[0;36mread_meta_graph_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/framework/meta_graph.py?line=631'>632</a>\u001b[0m meta_graph_def \u001b[39m=\u001b[39m meta_graph_pb2\u001b[39m.\u001b[39mMetaGraphDef()\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/framework/meta_graph.py?line=632'>633</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m file_io\u001b[39m.\u001b[39mfile_exists(filename):\n\u001b[1;32m--> <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/framework/meta_graph.py?line=633'>634</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFile does not exist. Received: \u001b[39m\u001b[39m{filename}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/framework/meta_graph.py?line=634'>635</a>\u001b[0m \u001b[39m# First try to read it as a binary file.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Acer/miniconda3/lib/site-packages/tensorflow/python/framework/meta_graph.py?line=635'>636</a>\u001b[0m \u001b[39mwith\u001b[39;00m file_io\u001b[39m.\u001b[39mFileIO(filename, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;31mOSError\u001b[0m: File does not exist. Received: {filename}."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "batch_size = 64\n",
    "n_samples = 10\n",
    "top_n_predictions = 5\n",
    "\n",
    "def test_model():\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.compat.v1.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.compat.v1.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('input_x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('output_y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "                # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        display_image_predictions(random_test_features, random_test_labels, random_test_predictions, top_n_predictions)\n",
    "\n",
    "\n",
    "test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fedc3f56c56c7a0bd22d9790ede6240a5ff04828cd6de4187b314c8c8833d35c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
